{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df64e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83815b2-5e5a-4a0e-ad9f-7b2f0b09788b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install lab_black\n",
    "#%load_ext lab_black\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4772b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb89b5d-ac8f-49a1-80ed-3be17153192a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "\n",
    "from src.utils import instantiate_from_config, get_device\n",
    "from src.utils.vis import save_sdf_as_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690ae8df-0703-4eed-94e8-0c75126d7118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "th.set_grad_enabled(False)\n",
    "device = get_device()\n",
    "\n",
    "    th.cuda.set_device(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3ab22a-d774-490f-980a-6e310bae128c",
   "metadata": {},
   "source": [
    "# Load Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1928db-db77-4b88-b36f-8613a810399e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen32_args_path = \"config/gen32/chair.yaml\"\n",
    "gen32_ckpt_path = \"results/gen32/chair.pth\"\n",
    "sr64_args_path = \"config/sr32_64/chair.yaml\"\n",
    "sr64_ckpt_path = \"results/sr32_64/chair.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcebfec-8f2b-42ae-9a55-e8517bee8cae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(gen32_args_path) as f:\n",
    "    args1 = EasyDict(yaml.safe_load(f))\n",
    "with open(sr64_args_path) as f:\n",
    "    args2 = EasyDict(yaml.safe_load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75f0c2-b8fe-4d80-8797-894a6d077f81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1 = instantiate_from_config(args1.model)\n",
    "model1 = model1.cuda() if th.cuda.is_available() else model1\n",
    "ckpt = th.load(gen32_ckpt_path, map_location=\"cpu\")\n",
    "model1.load_state_dict(ckpt[\"model_ema\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4f4dbd-6fca-409e-bbef-e85f7a066b74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2 = instantiate_from_config(args2.model)\n",
    "model2 = model2.cuda() if th.cuda.is_available() else model2\n",
    "ckpt = th.load(sr64_ckpt_path, map_location=\"cpu\")\n",
    "model2.load_state_dict(ckpt[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00efd984-8e93-4041-84cf-d70b3cd64bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddpm_sampler1 = instantiate_from_config(args1.ddpm.valid)\n",
    "ddpm_sampler2 = instantiate_from_config(args2.ddpm.valid)\n",
    "\n",
    "if th.cuda.is_available():\n",
    "    ddpm_sampler1, ddpm_sampler2 = ddpm_sampler1.cuda(), ddpm_sampler2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e87cc-6e85-4cc7-b41e-beac554f1d5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor1 = instantiate_from_config(args1.preprocessor, device)\n",
    "preprocessor2 = instantiate_from_config(args2.preprocessor, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c31d13-c9b5-42b3-bf7e-6819aee68b44",
   "metadata": {},
   "source": [
    "# Generate Low-Resolution ($32^3$)\n",
    "\n",
    "Generates 5 low-resolution samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e759bd-f309-48e5-8c1e-e7b8fb0dfa2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out1 = ddpm_sampler1.sample_ddim(model1, (3, 1, 32, 32, 32), show_pbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fceb1e-d513-40b5-8954-e766fb82808a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out1 = preprocessor1.destandardize(out1)\n",
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036976bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.vis import plot_sdfs\n",
    "plot_sdfs(list(out1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997dc3e6-1c88-4245-9bfb-909d8247c016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save as an obj file\n",
    "for i, out in enumerate(out1):\n",
    "    save_sdf_as_mesh(f\"gen32_{i}-stokes.obj\", out, safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f373ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cond = F.interpolate(out1, (64, 64, 64), mode=\"nearest\")\n",
    "lr_cond = preprocessor2.standardize(lr_cond, 0)\n",
    "out2 = ddpm_sampler2.sample_ddim(lambda x, t: model2(th.cat([lr_cond, x], 1), t), (out1.shape[0], 1, 64, 64, 64), show_pbar=True)\n",
    "\n",
    "out2 = preprocessor2.destandardize(out2, 1)\n",
    "\n",
    "for i, out in enumerate(out2):\n",
    "    save_sdf_as_mesh(f\"sr64_{i}-stokes.obj\", out, safe=True)\n",
    "\n",
    "plot_sdfs(list(out2), title=\"Super-resolution origianal samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8116937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inversion\n",
    "out1_inv = ddpm_sampler1.sample_ddim(model1, x_t = ddpm_sampler1.invert_ddim(model1, preprocessor1.standardize(out1), show_pbar=True, debug_plot=True), show_pbar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1_inv = preprocessor1.destandardize(out1_inv)\n",
    "out1_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda1a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as an obj file\n",
    "for i, out in enumerate(out1_inv):\n",
    "    save_sdf_as_mesh(f\"inv_gen32_{i}-stokes.obj\", out, safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d43c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute norm difference\n",
    "th.norm(out1 - out1_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sdfs(out1_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc4b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sdfs([out1, out1_inv], titles=[\"Original\", \"Predicted from inversion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aea0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del out1_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dfecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization \n",
    "def volume_estimates(sdfs, surface_threshold=1e-8): \n",
    "    # Get the dimensions of the SDF tensor\n",
    "    sdf_resolution = sdfs.shape[-1]\n",
    "    \n",
    "    # Compute the voxel size\n",
    "    # TODO remove voxel_size as we don't care about the scale\n",
    "    voxel_size = 1.0 / (sdf_resolution - 1)\n",
    "    \n",
    "    # Create a grid of voxel coordinates\n",
    "    x = th.linspace(0, 1, sdf_resolution)\n",
    "    y = th.linspace(0, 1, sdf_resolution)\n",
    "    z = th.linspace(0, 1, sdf_resolution)\n",
    "    x, y, z = th.meshgrid(x, y, z)\n",
    "    \n",
    "    # th the position vector and normal vector\n",
    "    r = th.stack([x, y, z], dim=-1)\n",
    "    with th.enable_grad():\n",
    "        sdfs.requires_grad_(True)\n",
    "        n = th.autograd.grad(sdfs, [x, y, z], create_graph=True)[0]\n",
    "        n = n / th.norm(n, dim=-1, keepdim=True)\n",
    "    \n",
    "    # Find the boundary voxels (where SDF is approximately 0)\n",
    "    boundary_mask = th.isclose(sdfs, th.zeros_like(sdfs), atol=surface_threshold)\n",
    "    boundary_indices = th.where(boundary_mask)\n",
    "\n",
    "    \n",
    "    # Compute the volume (Stokes' theorem)\n",
    "    volume = th.sum(r[boundary_indices] * n[boundary_indices] * voxel_size ** 2)\n",
    "    \n",
    "    return volume.item()\n",
    "\n",
    "\n",
    "def volume_estimates(sdfs, surface_threshold=1e-6): \n",
    "    # Get the size of the SDF tensor\n",
    "    batch_size, _, depth, height, width = sdfs.shape\n",
    "\n",
    "    # Compute the gradient of the SDF tensor\n",
    "    sdfs.requires_grad_(True)\n",
    "    n = th.autograd.grad(sdfs, (sdfs,), grad_outputs=th.ones_like(sdfs), create_graph=True)\n",
    "    n = th.stack(n, dim=1)  # Shape: (batch_size, 3, depth, height, width)\n",
    "    assert th.allclose(th.norm(n, dim=(-3, -2, -1)), 1)\n",
    "\n",
    "    # Compute the boundaries with a Gaussian activation function \n",
    "    gaussian_activation = lambda x, sigma: th.exp(- (x / sigma) ** 2)\n",
    "    sdfs_boundary = gaussian_activation(sdfs, surface_threshold)\n",
    "\n",
    "    # Compute the volume using Stokes' Theorem\n",
    "    volume = th.sum(n @ sdfs_boundary)\n",
    "\n",
    "\n",
    "\n",
    "def volume_estimates_loss_fn(xs, target_volumes, max_volume=1., reduction=\"none\"):\n",
    "    input_volumes = volume_estimates(xs) / max_volume\n",
    "    target_volumes = target_volumes / max_volume\n",
    "    loss = th.nn.MSELoss(reduction=reduction)(input_volumes, target_volumes)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc74e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "# Ensure the src module is in the path\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils.vis import plot_sdfs\n",
    "\n",
    "def generate_batched_sdf_tensor(batch_size, shape, centers, radii):\n",
    "    x = th.linspace(0, 1, shape[0])\n",
    "    y = th.linspace(0, 1, shape[1])\n",
    "    z = th.linspace(0, 1, shape[2])\n",
    "    X, Y, Z = th.meshgrid(x, y, z, indexing='ij')\n",
    "    sdf_batch = th.zeros((batch_size, *shape))\n",
    "    for i in range(batch_size):\n",
    "        sdf_batch[i] = th.sqrt((X - centers[i][0])**2 + (Y - centers[i][1])**2 + (Z - centers[i][2])**2) - radii[i]\n",
    "    return sdf_batch\n",
    "\n",
    "xx = generate_batched_sdf_tensor(4, (10, 10, 10), [(0.0, 0.0, 0.0), (0.5, 0.5, 0.5)]*2, list(range(1, 5)))\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1e0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sdfs(xx.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d3290",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.requires_grad_(True)\n",
    "nn = th.autograd.grad(xx, (xx,), grad_outputs=th.ones_like(xx), create_graph=True)\n",
    "nn = th.stack(nn, dim=1)\n",
    "nn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b9538e",
   "metadata": {},
   "source": [
    "***I don't think autograd can be used because there is no computational path between the input grid and the sdfs*** -- > gradient should be approximated with difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6878e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.autograd.grad(xx, th.meshgrid([th.linspace(0, 1, d) for d in xx.shape], indexing='ij'), create_graph=True)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0203f07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.requires_grad_(False)\n",
    "for g in th.gradient(xx, spacing=1/(xx.shape[-1]-1)): \n",
    "    print(g.shape)\n",
    "\n",
    "# TODO.. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d984b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1_std = preprocessor1.standardize(out1)\n",
    "\n",
    "target_volume_increment = 0.4\n",
    "t_optim_idx = 8\n",
    "x_edited, x_t, x_t_optim = ddpm_sampler1.ddim_sample_latent_optimization(\n",
    "    model1, \n",
    "    x_0=out1_std, \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std) * (1+target_volume_increment), \"max_volume\": out1.shape[-3:].numel()*preprocessor1.sdf_clip},\n",
    "    t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"t_optim\"\n",
    ").values()\n",
    "\n",
    "plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[out1, preprocessor1.destandardize(x_edited), th.abs(out1- preprocessor1.destandardize(x_edited))], \n",
    "    title = f\"Optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        [f\"Absolute difference between original and edited\" for i in range(out1.shape[0])]\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f86576",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = 0.4\n",
    "t_optim_idx = 5\n",
    "x_edited, x_t, x_t_optim = ddpm_sampler1.ddim_sample_latent_optimization(\n",
    "    model1, \n",
    "    x_0=out1_std, \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std) * (1+target_volume_increment), \"max_volume\": out1.shape[-3:].numel()*preprocessor1.sdf_clip},\n",
    "    t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"t_optim\"\n",
    ").values()\n",
    "\n",
    "plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[out1, preprocessor1.destandardize(x_edited), th.abs(out1- preprocessor1.destandardize(x_edited))], \n",
    "    title = f\"Optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        [f\"Absolute difference between original and edited\" for i in range(out1.shape[0])]\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6518c1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = 0.4\n",
    "t_optim_idx = 15\n",
    "x_edited, x_t, x_t_optim = ddpm_sampler1.ddim_sample_latent_optimization(\n",
    "    model1, \n",
    "    x_0=out1_std, \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std) * (1+target_volume_increment), \"max_volume\": out1.shape[-3:].numel()*preprocessor1.sdf_clip},\n",
    "    t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"t_optim\"\n",
    ").values()\n",
    "\n",
    "plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[out1, preprocessor1.destandardize(x_edited), th.abs(out1- preprocessor1.destandardize(x_edited))], \n",
    "    title = f\"Optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        [f\"Absolute difference between original and edited\" for i in range(out1.shape[0])]\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c534ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = 0.4\n",
    "t_optim_idx = 3\n",
    "x_edited, x_t, x_t_optim = ddpm_sampler1.ddim_sample_latent_optimization(\n",
    "    model1, \n",
    "    x_0=out1_std, \n",
    "    obj_fn=volume_estimates_loss_fn,\n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std) * (1+target_volume_increment), \"max_volume\": out1.shape[-3:].numel()*preprocessor1.sdf_clip},\n",
    "    t_optim_idx=t_optim_idx,\n",
    "    tgt_noise_level=\"zero\"\n",
    ").values()\n",
    "\n",
    "plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[out1, preprocessor1.destandardize(x_edited), th.abs(out1- preprocessor1.destandardize(x_edited))], \n",
    "    title = f\"Optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        [f\"Absolute difference between original and edited\" for i in range(out1.shape[0])]\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d1db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, out in enumerate(preprocessor1.destandardize(x_edited)):\n",
    "    save_sdf_as_mesh(f\"gen32_{i}_v40%incr_3steps-stokes.obj\", out, safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cond_edit = F.interpolate(preprocessor1.destandardize(x_edited), (64, 64, 64), mode=\"nearest\")\n",
    "lr_cond_edit = preprocessor2.standardize(lr_cond_edit, 0)\n",
    "x_edited_sr = ddpm_sampler2.sample_ddim(lambda x, t: model2(th.cat([lr_cond_edit, x], 1), t), (out1.shape[0], 1, 64, 64, 64), show_pbar=True)\n",
    "\n",
    "x_edited_sr = preprocessor2.destandardize(x_edited_sr, 1)\n",
    "\n",
    "for i, out in enumerate(x_edited_sr):\n",
    "    save_sdf_as_mesh(f\"sr64_{i}_v40%incr_3steps-stokes.obj\", out, safe=True)\n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[out2, x_edited_sr], \n",
    "    title = f\"Super-resolution results of optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original (SR) ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited (SR) ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d405e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = 0.4\n",
    "t_optim_idx = 5\n",
    "x_edited, x_t, x_t_optim = ddpm_sampler1.ddim_sample_latent_optimization(\n",
    "    model1, \n",
    "    x_0=out1_std, \n",
    "    obj_fn=volume_estimates_loss_fn,\n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std) * (1+target_volume_increment), \"max_volume\": out1.shape[-3:].numel()*preprocessor1.sdf_clip},\n",
    "    t_optim_idx=t_optim_idx,\n",
    "    tgt_noise_level=\"zero\"\n",
    ").values()\n",
    "\n",
    "plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[out1, preprocessor1.destandardize(x_edited), th.abs(out1- preprocessor1.destandardize(x_edited))], \n",
    "    title = f\"Optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        [f\"Absolute difference between original and edited\" for i in range(out1.shape[0])]\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8146365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it's better to don't use the normalized loss\n",
    "target_volume_increment = 0.4\n",
    "t_optim_idx = 5\n",
    "x_edited, x_t, x_t_optim = ddpm_sampler1.ddim_sample_latent_optimization(\n",
    "    model1, \n",
    "    x_0=out1_std, \n",
    "    obj_fn=volume_estimates_loss_fn,\n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std) * (1+target_volume_increment), \"max_volume\": 1},\n",
    "    t_optim_idx=t_optim_idx,\n",
    "    tgt_noise_level=\"zero\", \n",
    "    loss_threshold=10,\n",
    "    opt_kwargs={\"lr\": 1e-2}\n",
    ").values()\n",
    "\n",
    "plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[out1, preprocessor1.destandardize(x_edited), th.abs(out1- preprocessor1.destandardize(x_edited))], \n",
    "    title = f\"Optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        [f\"Absolute difference between original and edited\" for i in range(out1.shape[0])]\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07249330",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, out in enumerate(preprocessor1.destandardize(x_edited)):\n",
    "    save_sdf_as_mesh(f\"gen32_{i}_v40%incr_5steps-stokes.obj\", out, safe=True)\n",
    "\n",
    "lr_cond_edit = F.interpolate(preprocessor1.destandardize(x_edited), (64, 64, 64), mode=\"nearest\")\n",
    "lr_cond_edit = preprocessor2.standardize(lr_cond_edit, 0)\n",
    "x_edited_sr = ddpm_sampler2.sample_ddim(lambda x, t: model2(th.cat([lr_cond_edit, x], 1), t), (out1.shape[0], 1, 64, 64, 64), show_pbar=True)\n",
    "\n",
    "x_edited_sr = preprocessor2.destandardize(x_edited_sr, 1)\n",
    "\n",
    "for i, out in enumerate(x_edited_sr):\n",
    "    save_sdf_as_mesh(f\"sr64_{i}_v40%incr_5steps-stokes.obj\", out, safe=True)\n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[out2, x_edited_sr], \n",
    "    title = f\"Super-resolution results of optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original (SR) ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited (SR) ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c4f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = 0.4\n",
    "t_optim_idx = 8\n",
    "x_edited, x_t, x_t_optim = ddpm_sampler1.ddim_sample_latent_optimization(\n",
    "    model1, \n",
    "    x_0=out1_std, \n",
    "    obj_fn=volume_estimates_loss_fn,\n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std) * (1+target_volume_increment)},\n",
    "    t_optim_idx=t_optim_idx,\n",
    "    tgt_noise_level=\"zero\"\n",
    ")\n",
    "\n",
    "plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[out1, preprocessor1.destandardize(x_edited), th.abs(out1- preprocessor1.destandardize(x_edited))], \n",
    "    title = f\"Optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        [f\"Absolute difference between original and edited\" for i in range(out1.shape[0])]\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f71ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, out in enumerate(preprocessor1.destandardize(x_edited)):\n",
    "    save_sdf_as_mesh(f\"gen32_{i}_v40%incr_8steps-stokes.obj\", out, safe=True)\n",
    "\n",
    "lr_cond_edit = F.interpolate(preprocessor1.destandardize(x_edited), (64, 64, 64), mode=\"nearest\")\n",
    "lr_cond_edit = preprocessor2.standardize(lr_cond_edit, 0)\n",
    "x_edited_sr = ddpm_sampler2.sample_ddim(lambda x, t: model2(th.cat([lr_cond_edit, x], 1), t), (out1.shape[0], 1, 64, 64, 64), show_pbar=True)\n",
    "\n",
    "x_edited_sr = preprocessor2.destandardize(x_edited_sr, 1)\n",
    "\n",
    "for i, out in enumerate(x_edited_sr):\n",
    "    save_sdf_as_mesh(f\"sr64_{i}_v40%incr_8steps-stokes.obj\", out, safe=True)\n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[out2, x_edited_sr], \n",
    "    title = f\"Super-resolution results of optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original (SR) ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited (SR) ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3dc065",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = -0.4\n",
    "t_optim_idx = 3\n",
    "x_edited, x_t, x_t_optim = ddpm_sampler1.ddim_sample_latent_optimization(\n",
    "    model1, \n",
    "    x_0=out1_std, \n",
    "    obj_fn=volume_estimates_loss_fn,\n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std) * (1+target_volume_increment)},\n",
    "    t_optim_idx=t_optim_idx,\n",
    "    tgt_noise_level=\"zero\"\n",
    ").values()\n",
    "\n",
    "plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[out1, preprocessor1.destandardize(x_edited), th.abs(out1- preprocessor1.destandardize(x_edited))], \n",
    "    title = f\"Optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        [f\"Absolute difference between original and edited\" for i in range(out1.shape[0])]\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6398a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, out in enumerate(preprocessor1.destandardize(x_edited)):\n",
    "    save_sdf_as_mesh(f\"gen32_{i}_-v40%incr_3steps-stokes.obj\", out, safe=True)\n",
    "\n",
    "lr_cond_edit = F.interpolate(preprocessor1.destandardize(x_edited), (64, 64, 64), mode=\"nearest\")\n",
    "lr_cond_edit = preprocessor2.standardize(lr_cond_edit, 0)\n",
    "x_edited_sr = ddpm_sampler2.sample_ddim(lambda x, t: model2(th.cat([lr_cond_edit, x], 1), t), (out1.shape[0], 1, 64, 64, 64), show_pbar=True)\n",
    "\n",
    "x_edited_sr = preprocessor2.destandardize(x_edited_sr, 1)\n",
    "\n",
    "for i, out in enumerate(x_edited_sr):\n",
    "    save_sdf_as_mesh(f\"sr64_{i}_-v40%incr_3steps-stokes.obj\", out, safe=True)\n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[out2, x_edited_sr], \n",
    "    title = f\"Super-resolution results of optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original (SR) ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited (SR) ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964111f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it's better to don't use the normalized loss\n",
    "target_volume_increment = -0.4\n",
    "t_optim_idx = 3\n",
    "x_edited, x_t, x_t_optim = ddpm_sampler1.ddim_sample_latent_optimization(\n",
    "    model1, \n",
    "    x_0=out1_std, \n",
    "    obj_fn=volume_estimates_loss_fn,\n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std) * (1+target_volume_increment), \"max_volume\": 1},\n",
    "    t_optim_idx=t_optim_idx,\n",
    "    tgt_noise_level=\"zero\", \n",
    "    loss_threshold=10,\n",
    "    opt_kwargs={\"lr\": 1e-2}\n",
    ").values()\n",
    "\n",
    "plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[out1, preprocessor1.destandardize(x_edited), th.abs(out1- preprocessor1.destandardize(x_edited))], \n",
    "    title = f\"Optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        [f\"Absolute difference between original and edited\" for i in range(out1.shape[0])]\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac88018",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = -0.4\n",
    "t_optim_idx = 5\n",
    "x_edited, x_t, x_t_optim = ddpm_sampler1.ddim_sample_latent_optimization(\n",
    "    model1, \n",
    "    x_0=out1_std, \n",
    "    obj_fn=volume_estimates_loss_fn,\n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std) * (1+target_volume_increment)},\n",
    "    t_optim_idx=t_optim_idx,\n",
    "    tgt_noise_level=\"zero\"\n",
    ").values()\n",
    "\n",
    "plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[out1, preprocessor1.destandardize(x_edited), th.abs(out1- preprocessor1.destandardize(x_edited))], \n",
    "    title = f\"Optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        [f\"Absolute difference between original and edited\" for i in range(out1.shape[0])]\n",
    "        ]\n",
    ")\n",
    "for i, out in enumerate(preprocessor1.destandardize(x_edited)):\n",
    "    save_sdf_as_mesh(f\"gen32_{i}_-v40%incr_5steps-stokes.obj\", out, safe=True)\n",
    "\n",
    "lr_cond_edit = F.interpolate(preprocessor1.destandardize(x_edited), (64, 64, 64), mode=\"nearest\")\n",
    "lr_cond_edit = preprocessor2.standardize(lr_cond_edit, 0)\n",
    "x_edited_sr = ddpm_sampler2.sample_ddim(lambda x, t: model2(th.cat([lr_cond_edit, x], 1), t), (out1.shape[0], 1, 64, 64, 64), show_pbar=True)\n",
    "\n",
    "x_edited_sr = preprocessor2.destandardize(x_edited_sr, 1)\n",
    "\n",
    "for i, out in enumerate(x_edited_sr):\n",
    "    save_sdf_as_mesh(f\"sr64_{i}_-v40%incr_5steps-stokes.obj\", out, safe=True)\n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[out2, x_edited_sr], \n",
    "    title = f\"Super-resolution results of optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original (SR) ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited (SR) ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3101e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it's better to don't use the normalized loss\n",
    "target_volume_increment = -0.4\n",
    "t_optim_idx = 5\n",
    "x_edited, x_t, x_t_optim = ddpm_sampler1.ddim_sample_latent_optimization(\n",
    "    model1, \n",
    "    x_0=out1_std, \n",
    "    obj_fn=volume_estimates_loss_fn,\n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std) * (1+target_volume_increment), \"max_volume\": 1},\n",
    "    t_optim_idx=t_optim_idx,\n",
    "    tgt_noise_level=\"zero\", \n",
    "    loss_threshold=10,\n",
    "    opt_kwargs={\"lr\": 1e-2}\n",
    ").values()\n",
    "\n",
    "plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[out1, preprocessor1.destandardize(x_edited), th.abs(out1- preprocessor1.destandardize(x_edited))], \n",
    "    title = f\"Optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        [f\"Absolute difference between original and edited\" for i in range(out1.shape[0])]\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ca607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decrease the LR a bit\n",
    "target_volume_increment = -0.4\n",
    "t_optim_idx = 5\n",
    "x_edited, x_t, x_t_optim = ddpm_sampler1.ddim_sample_latent_optimization(\n",
    "    model1, \n",
    "    x_0=out1_std, \n",
    "    obj_fn=volume_estimates_loss_fn,\n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std) * (1+target_volume_increment), \"max_volume\": 1},\n",
    "    t_optim_idx=t_optim_idx,\n",
    "    tgt_noise_level=\"zero\", \n",
    "    loss_threshold=10,\n",
    "    opt_kwargs={\"lr\": .5e-2}\n",
    ").values()\n",
    "\n",
    "plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[out1, preprocessor1.destandardize(x_edited), th.abs(out1- preprocessor1.destandardize(x_edited))], \n",
    "    title = f\"Optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        [f\"Absolute difference between original and edited\" for i in range(out1.shape[0])]\n",
    "        ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-m1-3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
