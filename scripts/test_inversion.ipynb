{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df64e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39942b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83815b2-5e5a-4a0e-ad9f-7b2f0b09788b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install lab_black\n",
    "#%load_ext lab_black\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb89b5d-ac8f-49a1-80ed-3be17153192a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "\n",
    "from src.utils import instantiate_from_config, get_device\n",
    "from src.utils.vis import save_sdf_as_mesh, plot_sdfs\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690ae8df-0703-4eed-94e8-0c75126d7118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "th.set_grad_enabled(False)\n",
    "device = get_device()\n",
    "#device='cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1928db-db77-4b88-b36f-8613a810399e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen32_args_path = \"config/gen32/chair.yaml\"\n",
    "gen32_ckpt_path = \"results/gen32/chair.pth\"\n",
    "sr64_args_path = \"config/sr32_64/chair.yaml\"\n",
    "sr64_ckpt_path = \"results/sr32_64/chair.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcebfec-8f2b-42ae-9a55-e8517bee8cae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(gen32_args_path) as f:\n",
    "    args1 = EasyDict(yaml.safe_load(f))\n",
    "with open(sr64_args_path) as f:\n",
    "    args2 = EasyDict(yaml.safe_load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75f0c2-b8fe-4d80-8797-894a6d077f81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1 = instantiate_from_config(args1.model)\n",
    "ckpt = th.load(gen32_ckpt_path, map_location=device)\n",
    "model1.load_state_dict(ckpt[\"model_ema\"])\n",
    "model1 = model1.to(device)\n",
    "model1.eval()\n",
    "model1.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4f4dbd-6fca-409e-bbef-e85f7a066b74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2 = instantiate_from_config(args2.model)\n",
    "ckpt = th.load(sr64_ckpt_path, map_location=device)\n",
    "model2.load_state_dict(ckpt[\"model\"])\n",
    "model2 = model2.to(device)\n",
    "model2.eval()\n",
    "model2.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00efd984-8e93-4041-84cf-d70b3cd64bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddpm_sampler1 = instantiate_from_config(args1.ddpm.valid, device=device)\n",
    "ddpm_sampler2 = instantiate_from_config(args2.ddpm.valid, device=device)\n",
    "\n",
    "ddpm_sampler1, ddpm_sampler2 = ddpm_sampler1.to(device), ddpm_sampler2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e87cc-6e85-4cc7-b41e-beac554f1d5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor1 = instantiate_from_config(args1.preprocessor, device=device)\n",
    "preprocessor2 = instantiate_from_config(args2.preprocessor, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c31d13-c9b5-42b3-bf7e-6819aee68b44",
   "metadata": {},
   "source": [
    "# Generate Low-Resolution ($32^3$)\n",
    "\n",
    "Generates 5 low-resolution samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcd47ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDIMScheduler, DDIMInverseScheduler\n",
    "\n",
    "prediction_type_map = {\n",
    "    \"x_0\": \"sample\", \n",
    "    \"eps\": \"epsilon\"\n",
    "} \n",
    "\n",
    "ddim_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=args1.ddpm.valid.params.schedule_kwargs.n_timestep,\n",
    "    #beta_start=args1.ddpm.valid.params.schedule_kwargs.linear_start,\n",
    "    #beta_end=args1.ddpm.valid.params.schedule_kwargs.linear_end,\n",
    "    trained_betas=ddpm_sampler1.betas.cpu(),\n",
    "    beta_schedule=args1.ddpm.valid.params.schedule_kwargs.schedule,\n",
    "    prediction_type=prediction_type_map[args1.ddpm.valid.params.model_mean_type],\n",
    "    #timestep_spacing=\"linspace\",\n",
    "    set_alpha_to_one=False\n",
    ")\n",
    "ddim_scheduler.set_timesteps(num_inference_steps=args1.ddpm.valid.params.schedule_kwargs.ddim_S, device=device)\n",
    "#ddim_scheduler.set_timesteps(num_inference_steps=args1.ddpm.valid.params.schedule_kwargs.n_timestep-1, device=device) # TODO: to test if increasing the timesteps leads to better results\n",
    "\n",
    "ddim_inverse_scheduler = DDIMInverseScheduler(\n",
    "    num_train_timesteps=args1.ddpm.valid.params.schedule_kwargs.n_timestep,\n",
    "    #beta_start=args1.ddpm.valid.params.schedule_kwargs.linear_start,\n",
    "    #beta_end=args1.ddpm.valid.params.schedule_kwargs.linear_end,\n",
    "    trained_betas=ddpm_sampler1.betas.cpu(),\n",
    "    beta_schedule=args1.ddpm.valid.params.schedule_kwargs.schedule,\n",
    "    prediction_type=prediction_type_map[args1.ddpm.valid.params.model_mean_type],\n",
    "    #timestep_spacing=\"linspace\",\n",
    "    set_alpha_to_one=False\n",
    ")\n",
    "ddim_inverse_scheduler.set_timesteps(num_inference_steps=args1.ddpm.valid.params.schedule_kwargs.ddim_S, device=device)\n",
    "#ddim_inverse_scheduler.set_timesteps(num_inference_steps=args1.ddpm.valid.params.schedule_kwargs.n_timestep-1, device=device)\n",
    "\n",
    "\n",
    "def diffusers_sample(\n",
    "        noise_scheduler, \n",
    "        model, \n",
    "        shape=None, \n",
    "        x_t=None, \n",
    "        from_t_idx=0,\n",
    "        to_t_idx=None, \n",
    "        cond=None, \n",
    "        return_intermediates=False, \n",
    "        plot_debug=False, \n",
    "        log_every_t=5,\n",
    "        device=\"cuda\",\n",
    "        ddpm_indexing=True\n",
    "    ):\n",
    "    \"\"\"Sample from the diffusion model.\"\"\"\n",
    "    assert (shape is not None) != (x_t is not None), \"Either shape or x_t must be provided, but not both.\" \n",
    "\n",
    "    # initialize noise\n",
    "    samples = th.randn(shape[:1] + shape[2:], device=device).unsqueeze(1) if shape is not None else x_t\n",
    "    shape = samples.shape\n",
    "    if to_t_idx == 0: \n",
    "        to_t_idx = None # Until last value\n",
    "    total_steps = len(noise_scheduler.timesteps)\n",
    "    timesteps = noise_scheduler.timesteps[from_t_idx:to_t_idx]\n",
    "    timesteps = th.cat((noise_scheduler.timesteps[[0]], timesteps[:-1])) if (from_t_idx % total_steps == 0) else th.cat((noise_scheduler.timesteps[[from_t_idx-1]], timesteps[:-1]))\n",
    "    timesteps = timesteps.to(device)\n",
    "\n",
    "    intermediates = [samples]\n",
    "    #noise_levels = []\n",
    "    # sample iteratively\n",
    "    # as done in original SDF code\n",
    "    #sqrt_alphas_cumprod_prev = th.sqrt(th.cat((noise_scheduler.alphas_cumprod[[0]], noise_scheduler.alphas_cumprod))).to(device)\n",
    "    sqrt_alphas_cumprod_prev_ddpm = th.sqrt(th.cat((th.tensor([1.]), noise_scheduler.alphas_cumprod[:-1]))).to(device) # as it has been trained with 1 at first value\n",
    "    # Similarly to inversion\n",
    "    sqrt_alphas_cumprod_prev = th.sqrt(noise_scheduler.alphas_cumprod.to(device)[reversed(timesteps+1)]) # +1 shift as in ldm implementation (with only t results are similar)\n",
    "    #alphas_cumprod = sqrt_alphas_cumprod_prev[:-1]\n",
    "    #print(th.cat((ddim_alphas_cumprod[[0]], ddim_alphas_cumprod[:-1])))\n",
    "    if from_t_idx is not None: \n",
    "        idx_shift = from_t_idx if from_t_idx >= 0 else (total_steps + from_t_idx)\n",
    "    else:\n",
    "        idx_shift = 0\n",
    "    timesteps = timesteps[1:]\n",
    "    with th.no_grad():\n",
    "        for i, t in enumerate(tqdm(timesteps)):\n",
    "            # prev_timestep = t - noise_scheduler.config.num_train_timesteps // noise_scheduler.num_inference_steps\n",
    "            # noise_level = th.sqrt(noise_scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else noise_scheduler.final_alpha_cumprod)\n",
    "            #noise_levels.append(noise_level)\n",
    "\n",
    "            # as done in original SDF code\n",
    "            #noise_level = sqrt_alphas_cumprod_prev[t+1] # t+1 as in ldm code \n",
    "            index = total_steps - i - 1 - idx_shift\n",
    "            # noise_level = sqrt_alphas_cumprod_prev[[index]]\n",
    "            #print(th.cat((ddim_alphas_cumprod[[0]], ddim_alphas_cumprod[:-1]))[[index]])\n",
    "            if ddpm_indexing:\n",
    "                noise_level = sqrt_alphas_cumprod_prev_ddpm[t+1] # t+1 as in ldm code (with only t results are similar)\n",
    "            else:  \n",
    "                noise_level = sqrt_alphas_cumprod_prev[[index]]\n",
    "\n",
    "            #print(f\"Using DDPM indexing: {ddpm_indexing}\\t\", \"noise level: with DDPM indexing:\", sqrt_alphas_cumprod_prev_ddpm[t+1], \", with DDIM indexing:\", sqrt_alphas_cumprod_prev[[index]] )\n",
    "\n",
    "            pred = model(\n",
    "                samples, noise_level * th.ones(shape[0], device=device), c=cond\n",
    "            )\n",
    "            samples = noise_scheduler.step(pred, t, samples)\n",
    "            if i == len(timesteps)-1:\n",
    "                intermediates.append(samples)\n",
    "                if plot_debug: \n",
    "                    print(f'noise_level: {noise_level[0].item() if noise_level.numel() > 1 else noise_level.item() }')\n",
    "                    plot_sdfs(samples.prev_sample, title=f\"DeNoising - t={t}/{noise_scheduler.config.num_train_timesteps-1} (DDIM: t={index}/{noise_scheduler.num_inference_steps-1})\")\n",
    "            elif i % log_every_t == 0:\n",
    "                if return_intermediates:\n",
    "                    intermediates.append(samples)\n",
    "                if plot_debug: \n",
    "                    print(f'noise_level: {noise_level[0].item() if noise_level.numel() > 1 else noise_level.item() }')\n",
    "                    plot_sdfs(samples.prev_sample, title=f\"DeNoising - t={t}/{noise_scheduler.config.num_train_timesteps-1} (DDIM: t={index}/{noise_scheduler.num_inference_steps-1})\")\n",
    "            samples = samples.prev_sample\n",
    "    if return_intermediates:\n",
    "        return intermediates\n",
    "    return intermediates[-1]\n",
    "\n",
    "def diffusers_inverse_sample(\n",
    "        inverse_noise_scheduler, \n",
    "        model, \n",
    "        x_t,\n",
    "        from_t_idx=0,\n",
    "        to_t_idx=None,\n",
    "        cond=None, \n",
    "        return_intermediates=False, \n",
    "        log_every_t=5, \n",
    "        plot_debug=False,\n",
    "        device=\"cuda\", \n",
    "        ddpm_indexing=True\n",
    "    ):\n",
    "    \"\"\"Invert a sample to noise with the diffusion model.\"\"\"\n",
    "\n",
    "    samples = x_t\n",
    "    shape = samples.shape\n",
    "\n",
    "    intermediates = [samples]\n",
    "    # alphas_cumprod = inverse_noise_scheduler.alphas_cumprod.to(device)\n",
    "    if to_t_idx == 0: \n",
    "        to_t_idx = None # Until last value\n",
    "    total_steps = len(inverse_noise_scheduler.timesteps)\n",
    "    #timesteps = inverse_noise_scheduler.timesteps[from_t_idx:to_t_idx]\n",
    "    timesteps = th.cat((inverse_noise_scheduler.timesteps[from_t_idx+1:], inverse_noise_scheduler.timesteps[[-1]])) if (to_t_idx is None or to_t_idx % total_steps == 0) else inverse_noise_scheduler.timesteps[from_t_idx+1:to_t_idx+1]\n",
    "    timesteps = timesteps.to(device)\n",
    "\n",
    "    # as done in original SDF code\n",
    "    # TODO: TRY ONLY INDEXING sqrt_alphas_cumprod as next should be t (DOESN'T WORK )\n",
    "    #sqrt_alphas_cumprod_next = th.sqrt(th.cat((inverse_noise_scheduler.alphas_cumprod[1:], inverse_noise_scheduler.alphas_cumprod[[-1]]))).to(device)\n",
    "\n",
    "    # As in ddim_sampler.invert_ddim \n",
    "    sqrt_alphas_cumprod_next = th.sqrt(inverse_noise_scheduler.alphas_cumprod.to(device)[timesteps+1]) # +1 shift as in ldm implementation (with only t results are similar)\n",
    "    #sqrt_alphas_cumprod_next = th.sqrt(ddim_alphas_cumprod[1:])\n",
    "\n",
    "    # # Similar to ldm make_ddim_sampling_parameters\n",
    "    # TODO\n",
    "    # alphas_prev = th.sqrt(th.cat((inverse_noise_scheduler.alphas_cumprod[1:], inverse_noise_scheduler.alphas_cumprod[[-1]]))).to(device)\n",
    "\n",
    "    \n",
    "    # Same as sample\n",
    "    #sqrt_alphas_cumprod_ddpm = th.sqrt(inverse_noise_scheduler.alphas_cumprod).to(device) \n",
    "    sqrt_alphas_cumprod_next_ddpm = th.sqrt(th.cat((inverse_noise_scheduler.alphas_cumprod[1:], inverse_noise_scheduler.alphas_cumprod[[-1]]))).to(device)\n",
    "    #sqrt_alphas_cumprod_next_ddpm = sqrt_alphas_cumprod_next_ddpm[timesteps+1]\n",
    "\n",
    "\n",
    "    # sqrt_alphas_cumprod_next = th.sqrt(th.cat((alphas_cumprod[1:], alphas_cumprod[[-1]]))).to(device)\n",
    "    # #print(len(sqrt_alphas_cumprod_next))\n",
    "    if from_t_idx is not None: \n",
    "        idx_shift = from_t_idx if from_t_idx >= 0 else (total_steps + from_t_idx)\n",
    "    else:\n",
    "        idx_shift = 0\n",
    "    # sample iteratively\n",
    "    timesteps = inverse_noise_scheduler.timesteps[from_t_idx:to_t_idx]\n",
    "    with th.no_grad():\n",
    "        for i, t in enumerate(tqdm(timesteps)): #TODO maybe I should skip the last step? as right know im dpong .step() even at t=980 (DEBUG INSIDE STEP FUNCTION!!)\n",
    "            # prev_timestep = t\n",
    "            # timestep = min(t - inverse_noise_scheduler.config.num_train_timesteps // inverse_noise_scheduler.num_inference_steps, inverse_noise_scheduler.config.num_train_timesteps - 1)\n",
    "            # alpha_prod_t = inverse_noise_scheduler.alphas_cumprod[timestep] if timestep >= 0 else inverse_noise_scheduler.initial_alpha_cumprod\n",
    "            # alpha_prod_t_prev = inverse_noise_scheduler.alphas_cumprod[prev_timestep]\n",
    "            # noise_level = th.sqrt(alpha_prod_t_prev) # TODO: NOT SURE WHAT OF THE TWO IS CORRECT (alpha_prod_t or alpha_prod_t_prev) -- it should be alpha_prod_t_prev tho \n",
    "            # #noise_level = th.sqrt(alpha_prod_t)\n",
    "            index = i + idx_shift\n",
    "            #noise_level = ddim_alphas_cumprod[[index]]\n",
    "            #noise_level = sqrt_alphas_cumprod_ddpm[t+1] # t+1 as in ldm code \n",
    "            if ddpm_indexing:\n",
    "                noise_level = sqrt_alphas_cumprod_next_ddpm[t+1] # t+1 as in ldm code (with only t results are similar)\n",
    "                #noise_level = sqrt_alphas_cumprod_next_ddpm[index]\n",
    "            else:\n",
    "                noise_level = sqrt_alphas_cumprod_next[[index]]\n",
    "            \n",
    "            #print(f\"Using DDPM indexing: {ddpm_indexing}\\t\", \"noise level: with DDPM indexing:\", sqrt_alphas_cumprod_next_ddpm[t+1], \", with DDIM indexing:\", sqrt_alphas_cumprod_next[[index]] )\n",
    "\n",
    "            #noise_level = inverse_noise_scheduler.alphas_cumprod[t+1]\n",
    "\n",
    "\n",
    "            pred = model(\n",
    "                samples, noise_level * th.ones(shape[0], device=device), c=cond\n",
    "            )\n",
    "            samples = inverse_noise_scheduler.step(pred, t, samples)\n",
    "            if i == len(timesteps)-1:\n",
    "                intermediates.append(samples)\n",
    "                if plot_debug: \n",
    "                    print(f'noise_level: {noise_level[0].item() if noise_level.numel() > 1 else noise_level.item() }')\n",
    "                    plot_sdfs(samples.prev_sample, title=f\"Noising - t={t}/{inverse_noise_scheduler.config.num_train_timesteps-1} (DDIM: t={index}/{inverse_noise_scheduler.num_inference_steps-1})\")\n",
    "            elif i % log_every_t == 0:\n",
    "                if return_intermediates:\n",
    "                    intermediates.append(samples)\n",
    "                if plot_debug: \n",
    "                    print(f'noise_level: {noise_level[0].item() if noise_level.numel() > 1 else noise_level.item() }')\n",
    "                    plot_sdfs(samples.prev_sample, title=f\"Noising - t={t}/{inverse_noise_scheduler.config.num_train_timesteps-1} (DDIM: t={index}/{inverse_noise_scheduler.num_inference_steps-1})\")\n",
    "            samples = samples.prev_sample\n",
    "    if return_intermediates:\n",
    "        return intermediates\n",
    "    return intermediates[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.utils import seed_everything\n",
    "seed_everything(40)\n",
    "out1 = diffusers_sample(ddim_scheduler, model1, shape=(2, 1, 32, 32, 32), device=device, ddpm_indexing=False).prev_sample\n",
    "out1_diffusers = out1\n",
    "plot_sdfs(list(out1), title=\"Diffusers sampling -- using ddpm1_sampler betas\") \n",
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39427a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from diffusers import DDIMInverseScheduler\n",
    "\n",
    "num_train_timesteps=1000\n",
    "scheduler = DDIMInverseScheduler(num_train_timesteps=num_train_timesteps, timestep_spacing='leading', prediction_type=\"sample\")\n",
    "\n",
    "inference_step = 10\n",
    "scheduler.set_timesteps(inference_step)\n",
    "\n",
    "# Before fix: The previous timestep can become negative.\n",
    "previous_timesteps = torch.tensor([min(timestep - num_train_timesteps // inference_step, num_train_timesteps - 1) for timestep in scheduler.timesteps])\n",
    "print('Previous Timestep\\tCurrent timesteps')\n",
    "for prev, cur in zip(previous_timesteps, scheduler.timesteps):\n",
    "    print(f'{prev}\\t\\t\\t{cur}')\n",
    "\n",
    "# After fix\n",
    "previous_timesteps = torch.tensor([min(timestep + num_train_timesteps // inference_step, num_train_timesteps - 1) for timestep in scheduler.timesteps])\n",
    "print('Previous Timestep\\tCurrent timesteps')\n",
    "for prev, cur in zip(previous_timesteps, scheduler.timesteps):\n",
    "    print(f'{prev}\\t\\t\\t{cur}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdb1067",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST WITH https://github.com/huggingface/diffusers/issues/10695 modification: seems wrong (both with DDPM and DDIM indexing)\n",
    "\n",
    "# plot_sdfs(diffusers_sample(\n",
    "#     ddim_scheduler,\n",
    "#     model1, \n",
    "#     x_t = diffusers_inverse_sample(\n",
    "#         ddim_inverse_scheduler,\n",
    "#         model1,\n",
    "#         x_t=out1_diffusers,\n",
    "#         device=device,\n",
    "#         plot_debug=True,\n",
    "#         log_every_t=20,\n",
    "#         ddpm_indexing=True,\n",
    "#     ).prev_sample,\n",
    "#     device=device,\n",
    "#     plot_debug=True,\n",
    "#     log_every_t=20,\n",
    "#     ddpm_indexing=True\n",
    "# ).prev_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f895c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sdfs(diffusers_sample(\n",
    "    ddim_scheduler,\n",
    "    model1, \n",
    "    x_t = diffusers_inverse_sample(\n",
    "        ddim_inverse_scheduler,\n",
    "        model1,\n",
    "        x_t=out1_diffusers.to(device),\n",
    "        device=device,\n",
    "        plot_debug=True,\n",
    "        log_every_t=20,\n",
    "        ddpm_indexing=False,\n",
    "    ).prev_sample,\n",
    "    device=device,\n",
    "    plot_debug=True,\n",
    "    log_every_t=20,\n",
    "    ddpm_indexing=False,\n",
    ").prev_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c7b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sdfs(diffusers_sample(\n",
    "    ddim_scheduler,\n",
    "    model1, \n",
    "    x_t = diffusers_inverse_sample(\n",
    "        ddim_inverse_scheduler,\n",
    "        model1,\n",
    "        x_t=out1_diffusers.to(device),\n",
    "        device=device,\n",
    "        plot_debug=True,\n",
    "        log_every_t=5,\n",
    "        ddpm_indexing=False,\n",
    "        to_t_idx=10,\n",
    "    ).prev_sample,\n",
    "    device=device,\n",
    "    plot_debug=True,\n",
    "    log_every_t=5,\n",
    "    ddpm_indexing=False,\n",
    "    from_t_idx=-10,\n",
    ").prev_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb5dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(40)\n",
    "out1 = diffusers_sample(ddim_scheduler, model1, shape=(2, 1, 32, 32, 32), device=device, ddpm_indexing=True).prev_sample\n",
    "out1_diffusers = out1\n",
    "plot_sdfs(list(out1), title=\"Diffusers sampling -- using ddpm1_sampler betas\") \n",
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.equal(ddim_scheduler.alphas_cumprod, ddim_inverse_scheduler.alphas_cumprod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438bf928",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.equal(ddim_scheduler.betas.cpu(), ddpm_sampler1.betas.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a12f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.equal(ddim_scheduler.alphas_cumprod.cpu(), ddpm_sampler1.alphas_cumprod.cpu()), len(ddim_scheduler.alphas_cumprod.cpu()), len(ddpm_sampler1.alphas_cumprod.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d4241",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.norm(ddim_scheduler.alphas_cumprod.cpu() - ddpm_sampler1.alphas_cumprod.cpu()), max(ddim_scheduler.alphas_cumprod.cpu() - ddpm_sampler1.alphas_cumprod.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1703dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddim_scheduler.alphas_cumprod.cpu().dtype, ddpm_sampler1.alphas_cumprod.cpu().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65179c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sdfs(diffusers_sample(\n",
    "    ddim_scheduler,\n",
    "    model1, \n",
    "    x_t = diffusers_inverse_sample(\n",
    "        ddim_inverse_scheduler,\n",
    "        model1,\n",
    "        x_t=out1_diffusers.to(device),\n",
    "        device=device,\n",
    "        plot_debug=True,\n",
    "        log_every_t=20,\n",
    "        ddpm_indexing=True,\n",
    "    ).prev_sample,\n",
    "    device=device,\n",
    "    plot_debug=True,\n",
    "    log_every_t=20,\n",
    "    ddpm_indexing=True,\n",
    ").prev_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f9a7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sdfs(diffusers_sample(\n",
    "    ddim_scheduler,\n",
    "    model1, \n",
    "    x_t = diffusers_inverse_sample(\n",
    "        ddim_inverse_scheduler,\n",
    "        model1,\n",
    "        x_t=out1_diffusers.to(device),\n",
    "        device=device,\n",
    "        plot_debug=True,\n",
    "        log_every_t=5,\n",
    "        ddpm_indexing=True,\n",
    "        to_t_idx=10,\n",
    "    ).prev_sample,\n",
    "    device=device,\n",
    "    plot_debug=True,\n",
    "    log_every_t=5,\n",
    "    ddpm_indexing=True,\n",
    "    from_t_idx=-10,\n",
    ").prev_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e0ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training mode doesn't affect the inversion -- as expected\n",
    "# model1.train()\n",
    "# print(model1.training)\n",
    "# seed_everything(40)\n",
    "# out1 = ddpm_sampler1.sample_ddim(model1, shape=(2, 1, 32, 32, 32), show_pbar=True)\n",
    "# plot_sdfs(list(out1))\n",
    "# plot_sdfs(ddpm_sampler1.sample_ddim(\n",
    "#     model1, \n",
    "#     x_t = ddpm_sampler1.invert_ddim(\n",
    "#         model1,\n",
    "#         x_t=out1,\n",
    "#         show_pbar=True\n",
    "#     ),\n",
    "#     show_pbar=True\n",
    "# ))\n",
    "\n",
    "\n",
    "# model1.eval()\n",
    "# print(model1.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91830712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.utils import seed_everything\n",
    "seed_everything(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e759bd-f309-48e5-8c1e-e7b8fb0dfa2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out1 = ddpm_sampler1.sample_ddim(model1, shape=(2, 1, 32, 32, 32), show_pbar=True, debug_plot=True, log_every_t=20, ddpm_indexing=False)\n",
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a311560",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(40)\n",
    "out1 = ddpm_sampler1.sample_ddim(model1, shape=(2, 1, 32, 32, 32), show_pbar=True, debug_plot=True, log_every_t=20, ddpm_indexing=True)\n",
    "out1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee25bc54",
   "metadata": {},
   "source": [
    "Seems like ddpm_indexing=True yields better samples, but let's investigate further..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e594eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.utils import seed_everything\n",
    "seed_everything(40)\n",
    "five_samples_ddpm_indexing = ddpm_sampler1.sample_ddim(model1, shape=(5, 1, 32, 32, 32), show_pbar=True, debug_plot=True, log_every_t=20, ddpm_indexing=True)\n",
    "plot_sdfs(list(five_samples_ddpm_indexing), title=\"Samples generation (with DDPM indexing)\")\n",
    "plot_sdfs(list(\n",
    "    ddpm_sampler1.sample_ddim(\n",
    "        model1,\n",
    "        x_t=ddpm_sampler1.invert_ddim(model1, x_t=five_samples_ddpm_indexing, show_pbar=True, debug_plot=True, log_every_t=20, ddpm_indexing=True),\n",
    "        show_pbar=True,\n",
    "        debug_plot=True, log_every_t=20,\n",
    "        ddpm_indexing=True,\n",
    "    )),\n",
    "    title=\"Predicted samples from inversion (with DDPM indexing)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d40b3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.utils import seed_everything\n",
    "seed_everything(40)\n",
    "five_samples_ddpm_indexing = ddpm_sampler1.sample_ddim(model1, shape=(5, 1, 32, 32, 32), show_pbar=True, debug_plot=True, log_every_t=20, ddpm_indexing=False)\n",
    "plot_sdfs(list(five_samples_ddpm_indexing), title=\"Samples generation (with DDIM indexing)\")\n",
    "plot_sdfs(list(\n",
    "    ddpm_sampler1.sample_ddim(\n",
    "        model1,\n",
    "        x_t=ddpm_sampler1.invert_ddim(model1, x_t=five_samples_ddpm_indexing, show_pbar=True, debug_plot=True, log_every_t=20, ddpm_indexing=False),\n",
    "        show_pbar=True,\n",
    "        debug_plot=True, log_every_t=20,\n",
    "        ddpm_indexing=False,\n",
    "    )),\n",
    "    title=\"Predicted samples from inversion (with DDIM indexing)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c39c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.norm(out1_diffusers - out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fceb1e-d513-40b5-8954-e766fb82808a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out1 = preprocessor1.destandardize(out1)\n",
    "print(out1.mean(), out1.min(), out1.max(), out1.var())\n",
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbffe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1_std = preprocessor1.standardize(out1)\n",
    "print(out1_std.mean(), out1_std.min(), out1_std.max(), out1_std.var())\n",
    "out1_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036976bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.vis import plot_sdfs\n",
    "view_kwargs = {\"azim\": 30, \"elev\": 30, \"roll\": 0, \"vertical_axis\": \"y\"}\n",
    "plot_sdfs(list(out1), view_kwargs=view_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997dc3e6-1c88-4245-9bfb-909d8247c016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save as an obj file\n",
    "# for i, out in enumerate(out1):\n",
    "#     save_sdf_as_mesh(f\"gen32_{i}.obj\", out, safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f373ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cond = F.interpolate(out1, (64, 64, 64), mode=\"nearest\")\n",
    "lr_cond = preprocessor2.standardize(lr_cond, 0)\n",
    "out2 = ddpm_sampler1.sample_ddim(lambda x, t: model2(th.cat([lr_cond, x], 1), t), shape=(out1.shape[0], 1, 64, 64, 64), show_pbar=True)\n",
    "\n",
    "out2 = preprocessor2.destandardize(out2, 1)\n",
    "\n",
    "#for i, out in enumerate(out2):\n",
    "#    save_sdf_as_mesh(f\"sr64_{i}.obj\", out, safe=True)\n",
    "\n",
    "plot_sdfs(list(out2), title=\"Super-resolution origianal samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8116937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inversion\n",
    "out1_inv = ddpm_sampler1.sample_ddim(\n",
    "    model1, \n",
    "    x_t = ddpm_sampler1.invert_ddim(\n",
    "        #ddim_inverse_scheduler,\n",
    "        model1, \n",
    "        out1_std, \n",
    "        debug_plot=True, \n",
    "        log_every_t=10, \n",
    "        show_pbar=True,\n",
    "        #device=device\n",
    "    ), \n",
    "    return_intermediates=False, \n",
    "    debug_plot=True, \n",
    "    log_every_t=10, \n",
    "    show_pbar=True\n",
    ")\n",
    "#out1_inv = out1_invs[-1]\n",
    "plot_sdfs(out1_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1_inv = preprocessor1.destandardize(out1_inv)\n",
    "out1_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda1a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as an obj file\n",
    "# for i, out in enumerate(out1_inv):\n",
    "#     save_sdf_as_mesh(f\"inv_gen32_{i}.obj\", out, safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d43c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute norm difference\n",
    "th.norm(out1 - out1_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc4b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sdfs([out1, out1_inv], titles=[\"Original\", \"Predicted from inversion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc173f74",
   "metadata": {},
   "source": [
    "After model.eval() update, the inversion always lead to the same (basic) shape. Might be related to https://github.com/CompVis/latent-diffusion/issues/136 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDIMScheduler, DDIMInverseScheduler\n",
    "\n",
    "prediction_type_map = {\n",
    "    \"x_0\": \"sample\", \n",
    "    \"eps\": \"epsilon\"\n",
    "} \n",
    "\n",
    "ddim_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=args1.ddpm.valid.params.schedule_kwargs.n_timestep,\n",
    "    beta_start=args1.ddpm.valid.params.schedule_kwargs.linear_start,\n",
    "    beta_end=args1.ddpm.valid.params.schedule_kwargs.linear_end,\n",
    "    beta_schedule=args1.ddpm.valid.params.schedule_kwargs.schedule,\n",
    "    prediction_type=prediction_type_map[args1.ddpm.valid.params.model_mean_type],\n",
    "    #timestep_spacing=\"linspace\",\n",
    "    set_alpha_to_one=False\n",
    ")\n",
    "ddim_scheduler.set_timesteps(num_inference_steps=args1.ddpm.valid.params.schedule_kwargs.ddim_S, device=device)\n",
    "\n",
    "ddim_inverse_scheduler = DDIMInverseScheduler(\n",
    "    num_train_timesteps=args1.ddpm.valid.params.schedule_kwargs.n_timestep,\n",
    "    beta_start=args1.ddpm.valid.params.schedule_kwargs.linear_start,\n",
    "    beta_end=args1.ddpm.valid.params.schedule_kwargs.linear_end,\n",
    "    beta_schedule=args1.ddpm.valid.params.schedule_kwargs.schedule,\n",
    "    prediction_type=prediction_type_map[args1.ddpm.valid.params.model_mean_type],\n",
    "    #timestep_spacing=\"linspace\",\n",
    "    set_alpha_to_one=False\n",
    ")\n",
    "ddim_inverse_scheduler.set_timesteps(num_inference_steps=args1.ddpm.valid.params.schedule_kwargs.ddim_S, device=device)\n",
    "\n",
    "\n",
    "def diffusers_sample(\n",
    "        noise_scheduler, \n",
    "        model, \n",
    "        shape=None, \n",
    "        x_t=None, \n",
    "        from_t_idx=0,\n",
    "        to_t_idx=None, \n",
    "        cond=None, \n",
    "        return_intermediates=False, \n",
    "        plot_debug=False, \n",
    "        log_every_t=5,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "    \"\"\"Sample from the diffusion model.\"\"\"\n",
    "    assert (shape is not None) != (x_t is not None), \"Either shape or x_t must be provided, but not both.\" \n",
    "\n",
    "    # initialize noise\n",
    "    samples = th.randn(shape[:1] + shape[2:], device=device).unsqueeze(1) if shape is not None else x_t\n",
    "    shape = samples.shape\n",
    "\n",
    "    timesteps = noise_scheduler.timesteps.to(device)[from_t_idx:to_t_idx]\n",
    "\n",
    "    intermediates = [samples]\n",
    "    #noise_levels = []\n",
    "    # sample iteratively\n",
    "    # as done in original SDF code\n",
    "    #sqrt_alphas_cumprod_prev = th.sqrt(th.cat((noise_scheduler.alphas_cumprod[[0]], noise_scheduler.alphas_cumprod))).to(device)\n",
    "    sqrt_alphas_cumprod_prev = th.sqrt(th.cat((th.tensor([1]), noise_scheduler.alphas_cumprod))).to(device) # as it has been trained with 1 at first value\n",
    "    with th.no_grad():\n",
    "        for i, t in enumerate(tqdm(timesteps)):\n",
    "            # prev_timestep = t - noise_scheduler.config.num_train_timesteps // noise_scheduler.num_inference_steps\n",
    "            # noise_level = th.sqrt(noise_scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else noise_scheduler.final_alpha_cumprod)\n",
    "            #noise_levels.append(noise_level)\n",
    "\n",
    "            # as done in original SDF code\n",
    "            noise_level = sqrt_alphas_cumprod_prev[t+1] # t+1 as in ldm code \n",
    "\n",
    "            pred = model(\n",
    "                samples, noise_level * th.ones(shape[0], device=device), c=cond\n",
    "            )\n",
    "            samples = noise_scheduler.step(pred, t, samples)\n",
    "            if i == len(timesteps)-1:\n",
    "                intermediates.append(samples)\n",
    "                if plot_debug: \n",
    "                    plot_sdfs(samples.prev_sample, title=f\"Denoising - timestep {t} / {timesteps[0]} (DDIM timesteps: {len(timesteps)} out of {noise_scheduler.config.num_train_timesteps})\")\n",
    "            elif i % log_every_t == 0:\n",
    "                if return_intermediates:\n",
    "                    intermediates.append(samples)\n",
    "                if plot_debug: \n",
    "                    plot_sdfs(samples.prev_sample, title=f\"Denoising - timestep {t} / {timesteps[0]} (DDIM timesteps: {len(timesteps)} out of {noise_scheduler.config.num_train_timesteps})\")\n",
    "            samples = samples.prev_sample\n",
    "    if return_intermediates:\n",
    "        return intermediates\n",
    "    return intermediates[-1]\n",
    "\n",
    "def diffusers_inverse_sample(\n",
    "        inverse_noise_scheduler, \n",
    "        model, \n",
    "        x_t,\n",
    "        from_t_idx=0,\n",
    "        to_t_idx=None,\n",
    "        cond=None, \n",
    "        return_intermediates=False, \n",
    "        log_every_t=5, \n",
    "        plot_debug=False,\n",
    "        device=\"cuda\", \n",
    "    ):\n",
    "    \"\"\"Invert a sample to noise with the diffusion model.\"\"\"\n",
    "\n",
    "    samples = x_t\n",
    "    shape = samples.shape\n",
    "\n",
    "    intermediates = [samples]\n",
    "    # alphas_cumprod = inverse_noise_scheduler.alphas_cumprod.to(device)\n",
    "    timesteps = inverse_noise_scheduler.timesteps.to(device)[from_t_idx:to_t_idx]\n",
    "    # as done in original SDF code\n",
    "    # TODO: TRY ONLY INDEXING sqrt_alphas_cumprod as next should be t\n",
    "    #sqrt_alphas_cumprod_next = th.sqrt(th.cat((inverse_noise_scheduler.alphas_cumprod[1:], inverse_noise_scheduler.alphas_cumprod[[-1]]))).to(device)\n",
    "    # sqrt_alphas_cumprod_next = th.sqrt(th.cat((alphas_cumprod[1:], alphas_cumprod[[-1]]))).to(device)\n",
    "    # #print(len(sqrt_alphas_cumprod_next))\n",
    "    # sample iteratively\n",
    "    with th.no_grad():\n",
    "        for i, t in enumerate(tqdm(timesteps)):\n",
    "            # prev_timestep = t\n",
    "            # timestep = min(t - inverse_noise_scheduler.config.num_train_timesteps // inverse_noise_scheduler.num_inference_steps, inverse_noise_scheduler.config.num_train_timesteps - 1)\n",
    "            # alpha_prod_t = inverse_noise_scheduler.alphas_cumprod[timestep] if timestep >= 0 else inverse_noise_scheduler.initial_alpha_cumprod\n",
    "            # alpha_prod_t_prev = inverse_noise_scheduler.alphas_cumprod[prev_timestep]\n",
    "            # noise_level = th.sqrt(alpha_prod_t_prev) # TODO: NOT SURE WHAT OF THE TWO IS CORRECT (alpha_prod_t or alpha_prod_t_prev) -- it should be alpha_prod_t_prev tho \n",
    "            # #noise_level = th.sqrt(alpha_prod_t)\n",
    "            #noise_level = sqrt_alphas_cumprod_next[t+1] # t+1 as in ldm code \n",
    "            noise_level = th.sqrt(inverse_noise_scheduler.alphas_cumprod[t+1]) # t+1 as in ldm code\n",
    "            pred = model(\n",
    "                samples, noise_level * th.ones(shape[0], device=device), c=cond\n",
    "            )\n",
    "            samples = inverse_noise_scheduler.step(pred, t, samples)\n",
    "            if i == len(timesteps)-1:\n",
    "                intermediates.append(samples)\n",
    "                if plot_debug: \n",
    "                    plot_sdfs(samples.prev_sample, title=f\"Noising - timestep {t} / {timesteps[-1]} (DDIM timesteps: {len(timesteps)} out of {inverse_noise_scheduler.config.num_train_timesteps})\")\n",
    "            elif i % log_every_t == 0:\n",
    "                if return_intermediates:\n",
    "                    intermediates.append(samples)\n",
    "                if plot_debug: \n",
    "                    plot_sdfs(samples.prev_sample, title=f\"Noising - timestep {t} / {timesteps[-1]} (DDIM timesteps: {len(timesteps)} out of {inverse_noise_scheduler.config.num_train_timesteps})\")\n",
    "            samples = samples.prev_sample\n",
    "    if return_intermediates:\n",
    "        return intermediates\n",
    "    return intermediates[-1]\n",
    "\n",
    "seed_everything(40)\n",
    "out1_t = diffusers_sample(ddim_scheduler, model1, shape=(2, 1, 32, 32, 32), device=device, plot_debug=True, log_every_t=10).prev_sample\n",
    "plot_sdfs(list(out1_t))\n",
    "plot_sdfs(list(diffusers_sample(\n",
    "    ddim_scheduler, \n",
    "    model1,\n",
    "    x_t=diffusers_inverse_sample(\n",
    "        ddim_inverse_scheduler, \n",
    "        model1, \n",
    "        x_t=out1_std, \n",
    "        return_intermediates=False, \n",
    "        plot_debug=True, \n",
    "        log_every_t=10, \n",
    "        device=device).prev_sample,\n",
    "    plot_debug=True,\n",
    "    log_every_t=10,\n",
    "    device=device\n",
    ").prev_sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aea0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del out1_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b268d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.diffusion import identity\n",
    "from tqdm import tqdm\n",
    "from src.utils.vis import plot_sdfs\n",
    "    \n",
    "def ddim_sample_noise_guidance(\n",
    "    noise_scheduler,\n",
    "    inverse_noise_scheduler,\n",
    "    denoise_fn,\n",
    "    x_0,\n",
    "    from_t_optim_idx,\n",
    "    obj_fn,\n",
    "    obj_fn_args={},\n",
    "    tgt_noise_level = \"t_optim\",\n",
    "    clip_denoised=True,\n",
    "    denoise_kwargs={},\n",
    "    post_fn=identity,\n",
    "    #return_intermediates=False,\n",
    "    log_every_t=5,\n",
    "    show_pbar=False,\n",
    "    pbar_kwargs={},\n",
    "    opt_kwargs={\"lr\":1e-2, \"decay_fn\": identity}, \n",
    "    grad_clip_value=None,\n",
    "    plot_debug=False,\n",
    "):        \n",
    "    ddim_args = {\n",
    "        \"clip_denoised\": clip_denoised,\n",
    "        \"denoise_kwargs\": denoise_kwargs, \n",
    "        \"post_fn\": post_fn, \n",
    "        #\"return_intermediates\": return_intermediates, \n",
    "        \"log_every_t\": log_every_t, \n",
    "        \"show_pbar\": show_pbar, \n",
    "        \"pbar_kwargs\": pbar_kwargs\n",
    "    }\n",
    "\n",
    "    # Get the latent at the defined noise level\n",
    "    x_t = .invert_ddim(denoise_fn, x_0, to_t_idx=from_t_optim_idx, requires_grad=False, **ddim_args)\n",
    "    if plot_debug:  \n",
    "        plot_sdfs(x_t, title=f\"x_t inverted to timestep {self.ddim_timesteps[from_t_optim_idx]}\")\n",
    "\n",
    "    # Denoise it with guidance\n",
    "    # if from_t_optim_idx < 0:\n",
    "    #     idxs = range(from_t_optim_idx, 0)\n",
    "    # else:\n",
    "    #     idxs = range(from_t_optim_idx, len(self.ddim_timesteps))\n",
    "    idxs = range(from_t_optim_idx, 0, -1)\n",
    "    \n",
    "    #x_t_optim = x_t.clone()\n",
    "    #x_t_optim.requires_grad = True\n",
    "    #optimizer = th.optim.SGD([x_t_optim], **opt_kwargs)\n",
    "    with tqdm(idxs, desc=\"Latent optimization\") as pbar_idxs:\n",
    "        for i, t_idx in enumerate(pbar_idxs):\n",
    "            x_t.requires_grad_(True)\n",
    "            if tgt_noise_level == \"t_optim\":\n",
    "                tgt_pred = x_t\n",
    "            elif tgt_noise_level == \"zero\":\n",
    "                tgt_pred = self.sample_ddim(denoise_fn, x_t=x_t, from_t_idx=-t_idx, requires_grad=True, **ddim_args) # Too slow and computationally and memory intensive \n",
    "            elif tgt_noise_level == \"zero_pred\": \n",
    "                tgt_pred = self.sample_ddim(denoise_fn, x_t=x_t, from_t_idx=-(t_idx), to_t_idx=-(t_idx-1), requires_grad=True, return_intermediates=True, **ddim_args)[-1]\n",
    "                #tgt_pred = self.sample_ddim(denoise_fn, x_t=x_t, from_t_idx=-(t_optim_idx), to_t_idx=None, requires_grad=True, return_intermediates=True, **ddim_args)[1] # slow, unnecessary (tested to be the equal)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid noise level: available levels are \" + [\"t_optim\", \"zero\"] + \".\")\n",
    "            # optimizer.zero_grad(set_to_none=True)\n",
    "            # loss_i = obj_fn(tgt_pred, **obj_fn_args)\n",
    "            # loss_i.backward()\n",
    "            # if grad_clip_value is not None:\n",
    "            #     th.nn.utils.clip_grad_value_(x_t, grad_clip_value)\n",
    "            # optimizer.step()\n",
    "\n",
    "            #tgt_pred.requires_grad_(True)\n",
    "            with th.enable_grad():\n",
    "                loss_i = obj_fn(tgt_pred, **obj_fn_args) # tgt_pred is a function of x_t\n",
    "            grad_t = th.autograd.grad(loss_i, x_t, retain_graph=False)[0] # grad of loss wrt to x_t\n",
    "\n",
    "            x_t = self.sample_ddim(denoise_fn, x_t=x_t, from_t_idx=-(t_idx), to_t_idx=-(t_idx-1), requires_grad=False, **ddim_args) # x_{t-1}\n",
    "            decay_fn = opt_kwargs.get(\"decay_fn\", identity)\n",
    "            x_t = x_t - decay_fn(i) * opt_kwargs[\"lr\"] * grad_t\n",
    "            x_t.grad = None\n",
    "            loss_i.grad = None\n",
    "            tgt_pred.grad = None\n",
    "\n",
    "            pbar_idxs.set_postfix({\"Loss (mean)\": th.mean(loss_i).item()})\n",
    "\n",
    "            if plot_debug:\n",
    "                plot_sdfs([tgt_pred, x_t], title=f\"Optimization step {i} at timestep {self.ddim_timesteps[t_idx]}\", titles=[f\"Target shape (target type: \\\"{tgt_noise_level}\\\")\", \"Optimized shape\"])\n",
    "\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dfecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization \n",
    "def volume_estimates(sdfs, dx=1., dy=1., dz=1.): \n",
    "    #inside_mask = (-sdfs) > 0\n",
    "    #volume_estimates = th.sum(inside_mask.float(), dim=list(range(1, sdfs.ndim)))\n",
    "    volume_estimates_activation = th.sum(StraightThroughEstimator()(-sdfs), dim=list(range(1, sdfs.ndim)))\n",
    "    #assert th.allclose(volume_estimates, volume_estimates_bin_activation), f\"using mask: {volume_estimates}, with activation function {volume_estimates_bin_activation}\"\n",
    "    return volume_estimates_activation * dx * dy * dz\n",
    "\n",
    "class STEFunction(th.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return (input > 0).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return F.hardtanh(grad_output)\n",
    "\n",
    "class StraightThroughEstimator(th.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StraightThroughEstimator, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = STEFunction.apply(x)\n",
    "        return x\n",
    " \n",
    "\n",
    "def volume_estimates_loss_fn(xs, target_volumes, max_volume=1., grad_var_reg_weight=0, tot_variation_reg_weight=0):\n",
    "    input_volumes = volume_estimates(xs) / max_volume\n",
    "    target_volumes = target_volumes / max_volume\n",
    "    loss = th.nn.MSELoss()(input_volumes, target_volumes)\n",
    "    if grad_var_reg_weight > 0:\n",
    "        loss += grad_var_reg_weight * th.var(xs.grad, dim=list(range(1, xs.ndim)))\n",
    "    if tot_variation_reg_weight > 0:\n",
    "        loss += tot_variation_reg_weight * tot_variation(xs)\n",
    "    return loss \n",
    "\n",
    "def tot_variation(sdfs, weight=1.):       \n",
    "    tv_x = ((sdfs[:,:,1:,:,:] - sdfs[:,:,:-1,:,:]).pow(2)).sum()\n",
    "    tv_y = ((sdfs[:,:,:,1:,:] - sdfs[:,:,:,:-1,:]).pow(2)).sum()    \n",
    "    tv_z = ((sdfs[:,:,:,:,1:] - sdfs[:,:,:,:,:-1]).pow(2)).sum()\n",
    "    return tv_x + tv_y + tv_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8913e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = -0.062\n",
    "plot_sdfs(\n",
    "    [out1, out1 + shift], \n",
    "    titles=[\n",
    "        [f\"Original \\n $V={volume_estimates(out_i).item()}$\" for out_i in out1], \n",
    "        [f\"Shifting the sdfs by ${shift}$ \\n $V={volume_estimates(out_i_shift).item()}$ (incr.: ${((volume_estimates(out_i_shift).item() - volume_estimates(out_i).item())/volume_estimates(out_i).item()):.2f}$)\" for out_i_shift, out_i in zip(out1+shift, out1)], \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d984b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = 0.9\n",
    "t_optim_idx = 15\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std, \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"t_optim\",\n",
    "    opt_kwargs={\"lr\":1e-2}, \n",
    "\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1, \n",
    "        preprocessor1.destandardize(x_edited), \n",
    "        #th.abs(out1- preprocessor1.destandardize(x_edited))\n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at the same timestep\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        #[f\"Absolute difference between original and edited\" for i in range(out1.shape[0])]\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = 0.9\n",
    "t_optim_idx = 8\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std, \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"t_optim\",\n",
    "    opt_kwargs={\"lr\":1e-2}, \n",
    "\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1, \n",
    "        preprocessor1.destandardize(x_edited), \n",
    "        #th.abs(out1- preprocessor1.destandardize(x_edited))\n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at the same timestep\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        #[f\"Absolute difference between original and edited\" for i in range(out1.shape[0])]\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d9094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = 1.27\n",
    "t_optim_idx = 7\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std[0:1], \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std[0:1]) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"zero\",\n",
    "    opt_kwargs={\"lr\":1e-1}, \n",
    "    plot_debug=True,\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[0:1], \n",
    "        preprocessor1.destandardize(x_edited[0:1]), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1[0:1].shape[0])], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited)[i].item():.2f}, V_{{target}}= {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1[0:1].shape[0])],\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f589ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxxx = ddpm_sampler1.invert_ddim(model1, x_t=out1_std[0:1], to_t_idx=5, debug_plot=True, log_every_t=1, show_pbar=True)\n",
    "ddpm_sampler1.sample_ddim(model1, x_t=xxxx, from_t_idx=-5, debug_plot=True, log_every_t=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd33ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = ddpm_sampler1.invert_ddim(model1, x_t=out1_std[0:1], to_t_idx=None, debug_plot=True, log_every_t=10, show_pbar=True)\n",
    "\n",
    "ddpm_sampler1.sample_ddim(model1, x_t=xx, debug_plot=True, log_every_t=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db0540",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = th.vstack((th.randn_like(out1[0:1]), th.randn_like(out1[0:1])))\n",
    "\n",
    "ddpm_sampler1.sample_ddim(model1, x_t=xx, debug_plot=True, log_every_t=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e0b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDIMScheduler, DDIMInverseScheduler\n",
    "\n",
    "prediction_type_map = {\n",
    "    \"x_0\": \"sample\", \n",
    "    \"eps\": \"epsilon\"\n",
    "} \n",
    "\n",
    "ddim_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=args1.ddpm.valid.params.schedule_kwargs.n_timestep,\n",
    "    beta_start=args1.ddpm.valid.params.schedule_kwargs.linear_start,\n",
    "    beta_end=args1.ddpm.valid.params.schedule_kwargs.linear_end,\n",
    "    beta_schedule=args1.ddpm.valid.params.schedule_kwargs.schedule,\n",
    "    prediction_type=prediction_type_map[args1.ddpm.valid.params.model_mean_type],\n",
    "    #timestep_spacing=\"linspace\",\n",
    "    set_alpha_to_one=False\n",
    ")\n",
    "ddim_scheduler.set_timesteps(num_inference_steps=args1.ddpm.valid.params.schedule_kwargs.ddim_S, device=device)\n",
    "\n",
    "ddim_inverse_scheduler = DDIMInverseScheduler(\n",
    "    num_train_timesteps=args1.ddpm.valid.params.schedule_kwargs.n_timestep,\n",
    "    beta_start=args1.ddpm.valid.params.schedule_kwargs.linear_start,\n",
    "    beta_end=args1.ddpm.valid.params.schedule_kwargs.linear_end,\n",
    "    beta_schedule=args1.ddpm.valid.params.schedule_kwargs.schedule,\n",
    "    prediction_type=prediction_type_map[args1.ddpm.valid.params.model_mean_type],\n",
    "    #timestep_spacing=\"linspace\",\n",
    "    set_alpha_to_one=False\n",
    ")\n",
    "ddim_inverse_scheduler.set_timesteps(num_inference_steps=args1.ddpm.valid.params.schedule_kwargs.ddim_S, device=device)\n",
    "\n",
    "\n",
    "def diffusers_sample(noise_scheduler, model, shape=None, x_t=None, device=\"cuda\", cond_signal=None, cond=None, return_intermediates=False, log_every_t=5):\n",
    "    \"\"\"Sample from the diffusion model.\"\"\"\n",
    "    assert (shape is not None) != (x_t is not None), \"Either shape or x_t must be provided, but not both.\" \n",
    "\n",
    "    # initialize noise\n",
    "    samples = th.randn(shape, device=device) if shape is not None else x_t\n",
    "    shape = samples.shape\n",
    "\n",
    "    intermediates = [samples]\n",
    "    # sample iteratively\n",
    "    sqrt_alphas_cumprod_prev = th.sqrt(th.cat((noise_scheduler.alphas_cumprod[[0]], noise_scheduler.alphas_cumprod))).to(device)\n",
    "    from tqdm import tqdm\n",
    "    with th.no_grad():\n",
    "        for i, t in enumerate(tqdm(noise_scheduler.timesteps)):\n",
    "            pred = model(\n",
    "                samples, sqrt_alphas_cumprod_prev[t] * th.ones(shape[0], device=device), cond_signal, cond\n",
    "            )\n",
    "            samples = noise_scheduler.step(pred, t, samples).prev_sample\n",
    "            if return_intermediates and (i % log_every_t == 0 or i == len(noise_scheduler.timesteps)-1):\n",
    "                intermediates.append(samples)\n",
    "    if return_intermediates:\n",
    "        return intermediates\n",
    "    return samples\n",
    "\n",
    "def diffusers_inverse_sample(noise_scheduler, model, samples, device, cond_signal=None, cond=None, return_intermediates=False, log_every_t=5, plot_debug=False):\n",
    "    \"\"\"Invert a sample to noise with the diffusion model.\"\"\"\n",
    "\n",
    "    intermediates = [samples]\n",
    "    alphas_cumprod = noise_scheduler.alphas_cumprod.to(device)\n",
    "    timesteps = noise_scheduler.timesteps.to(device)\n",
    "    sqrt_alphas_cumprod_next = th.sqrt(th.cat((alphas_cumprod[1:], alphas_cumprod[[-1]]))).to(device)\n",
    "    #print(len(sqrt_alphas_cumprod_next))\n",
    "    # sample iteratively\n",
    "    from tqdm import tqdm\n",
    "    with th.no_grad():\n",
    "        for i, t in enumerate(tqdm(timesteps)):\n",
    "            pred = model(\n",
    "                samples, sqrt_alphas_cumprod_next[t] * th.ones(samples.shape[0], device=device), cond_signal, cond\n",
    "            )\n",
    "            samples = noise_scheduler.step(pred, t, samples).prev_sample\n",
    "            if (i % log_every_t == 0 or i == len(timesteps)-1):\n",
    "                if return_intermediates:\n",
    "                    intermediates.append(samples)\n",
    "                if plot_debug: \n",
    "                    plot_sdfs(samples, title=f\"Denoising - timestep {t} / {timesteps[-1]} (DDIM timesteps: {len(timesteps)} out of {noise_scheduler.config.num_train_timesteps})\")\n",
    "    if return_intermediates:\n",
    "        return intermediates\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344fc4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_interm = diffusers_sample(ddim_scheduler, model1, (1, 1, 32, 32, 32), device=device, return_intermediates=True, log_every_t=3)\n",
    "plot_sdfs(sample_interm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358253d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_interm = diffusers_inverse_sample(ddim_inverse_scheduler, model1, sample_interm[-1].to(device), device=device, return_intermediates=True, plot_debug=True, log_every_t=3)\n",
    "#plot_sdfs(inverse_interm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df3bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_interm_from_inverse = diffusers_sample(ddim_scheduler, model1, x_t=inverse_interm[-1], device=device, return_intermediates=True)\n",
    "plot_sdfs(sample_interm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafdc6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = 1.27\n",
    "t_optim_idx = 3\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std[0:1], \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std[0:1]) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"zero\",\n",
    "    opt_kwargs={\"lr\":1e-1}, \n",
    "    plot_debug=True,\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[0:1], \n",
    "        preprocessor1.destandardize(x_edited[0:1]), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1[0:1].shape[0])], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited)[i].item():.2f}, V_{{target}}= {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1[0:1].shape[0])],\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c4ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = 0.71\n",
    "t_optim_idx = 7\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std[1:2],  \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std[1:2]) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"zero\",\n",
    "    opt_kwargs={\"lr\":1e-1}, \n",
    "    plot_debug=True\n",
    "\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[1:2], \n",
    "        preprocessor1.destandardize(x_edited), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_i_std).item():.2f}$)\" for out1_i_std in out1_std[1:2]], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited_i).item():.2f}, V_{{target}}= {volume_estimates(out1_i_std).item() * (1+target_volume_increment):.2f})$\" for (out1_i_std, x_edited_i) in zip(out1_std[1:2], x_edited)],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e67a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = 0.71\n",
    "t_optim_idx = 3\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std[1:2],  \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std[1:2]) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"zero\",\n",
    "    opt_kwargs={\"lr\":1e-1}, \n",
    "    plot_debug=True\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[1:2], \n",
    "        preprocessor1.destandardize(x_edited), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_i_std).item():.2f}$)\" for out1_i_std in out1_std[1:2]], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited_i).item():.2f}, V_{{target}}= {volume_estimates(out1_i_std).item() * (1+target_volume_increment):.2f})$\" for (out1_i_std, x_edited_i) in zip(out1_std[1:2], x_edited)],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602e428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = 1.27\n",
    "t_optim_idx = 7\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std[0:1], \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std[0:1]) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"zero\",\n",
    "    opt_kwargs={\"lr\":1, \"decay_fn\": lambda x: th.exp(th.tensor(-x))}, \n",
    "    plot_debug=False\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[0:1], \n",
    "        preprocessor1.destandardize(x_edited[0:1]), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1[0:1].shape[0])], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited)[i].item():.2f}, V_{{target}}= {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1[0:1].shape[0])],\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aed9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = 0.71\n",
    "t_optim_idx = 7\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std[1:2],  \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std[1:2]) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"zero\",\n",
    "    opt_kwargs={\"lr\":1, \"decay_fn\": lambda x: th.exp(th.tensor(-x))},  \n",
    "    plot_debug=False,\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[1:2], \n",
    "        preprocessor1.destandardize(x_edited), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_i_std).item():.2f}$)\" for out1_i_std in out1_std[1:2]], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited_i).item():.2f}, V_{{target}}= {volume_estimates(out1_i_std).item() * (1+target_volume_increment):.2f})$\" for (out1_i_std, x_edited_i) in zip(out1_std[1:2], x_edited)],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c62ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = 1.27\n",
    "t_optim_idx = 7\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std[0:1], \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std[0:1]) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"zero_pred\",\n",
    "    opt_kwargs={\"lr\":1e-1}, \n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[0:1], \n",
    "        preprocessor1.destandardize(x_edited[0:1]), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume of the PREDICTED $x_0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1[0:1].shape[0])], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited)[i].item():.2f}, V_{{target}}= {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1[0:1].shape[0])],\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669e2e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = 0.71\n",
    "t_optim_idx = 7\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std[1:2],  \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std[1:2]) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"zero_pred\",\n",
    "    opt_kwargs={\"lr\":1e-1}, \n",
    "    plot_debug=True\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[1:2], \n",
    "        preprocessor1.destandardize(x_edited), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume of the PREDICTED $x_0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_i_std).item():.2f}$)\" for out1_i_std in out1_std[1:2]], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited_i).item():.2f}, V_{{target}}= {volume_estimates(out1_i_std).item() * (1+target_volume_increment):.2f})$\" for (out1_i_std, x_edited_i) in zip(out1_std[1:2], x_edited)],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41bc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = 1.27\n",
    "t_optim_idx = 8\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std[0:1], \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std[0:1]) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"zero_pred\",\n",
    "    opt_kwargs={\"lr\":1e-1}, \n",
    "    plot_debug=True\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[0:1], \n",
    "        preprocessor1.destandardize(x_edited[0:1]), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1[0:1].shape[0])], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited)[i].item():.2f}, V_{{target}}= {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1[0:1].shape[0])],\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d9f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = 0.71\n",
    "t_optim_idx = 8\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std[1:2],  \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std[1:2]) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"zero_pred\",\n",
    "    opt_kwargs={\"lr\":1e-1}, \n",
    "    plot_debug=True\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[1:2], \n",
    "        preprocessor1.destandardize(x_edited), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_i_std).item():.2f}$)\" for out1_i_std in out1_std[1:2]], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited_i).item():.2f}, V_{{target}}= {volume_estimates(out1_i_std).item() * (1+target_volume_increment):.2f})$\" for (out1_i_std, x_edited_i) in zip(out1_std[1:2], x_edited)],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db4eb90",
   "metadata": {},
   "source": [
    "DONE UNTIL HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdded417",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = -0.062\n",
    "plot_sdfs(\n",
    "    [out1, out1 + shift], \n",
    "    titles=[\n",
    "        [f\"Original \\n $V={volume_estimates(out_i).item()}$\" for out_i in out1], \n",
    "        [f\"Shifting the sdfs by ${shift}$ \\n $V={volume_estimates(out_i_shift).item()}$ (incr.: ${((volume_estimates(out_i_shift).item() - volume_estimates(out_i).item())/volume_estimates(out_i).item()):.2f}$)\" for out_i_shift, out_i in zip(out1+shift, out1)], \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd209c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = 1.27\n",
    "t_optim_idx = 7\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std[0:1], \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std[0:1]) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"zero\",\n",
    "    opt_kwargs={\"lr\":1e-1}, \n",
    "\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[0:1], \n",
    "        preprocessor1.destandardize(x_edited[0:1]), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1[0:1].shape[0])], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited)[i].item():.2f}, V_{{target}}= {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1[0:1].shape[0])],\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80269cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = 0.9\n",
    "t_optim_idx = 7\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std[1:2], \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std[1:2]) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"zero\",\n",
    "    opt_kwargs={\"lr\":1e-1}, \n",
    "\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[1:2], \n",
    "        preprocessor1.destandardize(x_edited), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_i_std).item():.2f}$)\" for out1_i_std in out1_std[1:2]], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited_i).item():.2f}, V_{{target}}= {volume_estimates(out1_i_std).item() * (1+target_volume_increment):.2f})$\" for (out1_i_std, x_edited_i) in zip(out1_std[1:2], x_edited)],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d9002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[1:2], \n",
    "        preprocessor1.destandardize(x_edited), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_i_std).item():.2f}$)\" for out1_i_std in out1_std[1:2]], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited_i).item():.2f}, V_{{target}}= {volume_estimates(out1_i_std).item() * (1+target_volume_increment):.2f})$\" for (out1_i_std, x_edited_i) in zip(out1_std[1:2], x_edited)],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = -0.059\n",
    "plot_sdfs(\n",
    "    [out1, out1 + shift], \n",
    "    titles=[\n",
    "        [f\"Original \\n $V={volume_estimates(out_i).item()}$\" for out_i in out1], \n",
    "        [f\"Shifting the sdfs by ${shift}$ \\n $V={volume_estimates(out_i_shift).item()}$ (incr.: ${((volume_estimates(out_i_shift).item() - volume_estimates(out_i).item())/volume_estimates(out_i).item()):.2f}$)\" for out_i_shift, out_i in zip(out1+shift, out1)], \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ecc18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = +0.055\n",
    "plot_sdfs(\n",
    "    [out1, out1 + shift], \n",
    "    titles=[\n",
    "        [f\"Original \\n $V={volume_estimates(out_i).item()}$\" for out_i in out1], \n",
    "        [f\"Shifting the sdfs by ${shift}$ \\n $V={volume_estimates(out_i_shift).item()}$ (incr.: ${((volume_estimates(out_i_shift).item() - volume_estimates(out_i).item())/volume_estimates(out_i).item()):.2f}$)\" for out_i_shift, out_i in zip(out1+shift, out1)], \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db8ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = -0.57\n",
    "t_optim_idx = 7\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std[0:1], \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std[0:1]) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"zero\",\n",
    "    opt_kwargs={\"lr\":1e-1}, \n",
    "\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[0:1], \n",
    "        preprocessor1.destandardize(x_edited), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_i_std).item():.2f}$)\" for out1_i_std in out1_std[0:1]], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited_i).item():.2f}, V_{{target}}= {volume_estimates(out1_i_std).item() * (1+target_volume_increment):.2f})$\" for (out1_i_std, x_edited_i) in zip(out1_std[0:1], x_edited)],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd8f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = -0.38\n",
    "t_optim_idx = 7\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std[1:2], \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std[1:2]) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"zero\",\n",
    "    opt_kwargs={\"lr\":1e-1}, \n",
    "\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[1:2], \n",
    "        preprocessor1.destandardize(x_edited), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_i_std).item():.2f}$)\" for out1_i_std in out1_std[1:2]], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited_i).item():.2f}, V_{{target}}= {volume_estimates(out1_i_std).item() * (1+target_volume_increment):.2f})$\" for (out1_i_std, x_edited_i) in zip(out1_std[1:2], x_edited)],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ba762",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediates_debug = ddpm_sampler1.sample_ddim(model1, (3, 1, 32, 32, 32), show_pbar=True, return_intermediates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6827e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_every_t = 5\n",
    "plot_sdfs(intermediates_debug, titles=[f\"Predicted $x_0$ at DDIM timestep ${t*log_every_t}$\" for t in range(len(intermediates_debug))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2444e737",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = -0.38\n",
    "t_optim_idx = 10\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std[1:2], \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std[1:2]) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"zero\",\n",
    "    opt_kwargs={\"lr\":1e-1}, \n",
    "\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[1:2], \n",
    "        preprocessor1.destandardize(x_edited), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_i_std).item():.2f}$)\" for out1_i_std in out1_std[1:2]], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited_i).item():.2f}, V_{{target}}= {volume_estimates(out1_i_std).item() * (1+target_volume_increment):.2f})$\" for (out1_i_std, x_edited_i) in zip(out1_std[1:2], x_edited)],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae88979",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = -0.38\n",
    "t_optim_idx = 7\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std[1:2], \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std[1:2]) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"zero\",\n",
    "    opt_kwargs={\"lr\":1e-1}, \n",
    "\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[1:2], \n",
    "        preprocessor1.destandardize(x_edited), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_i_std).item():.2f}$)\" for out1_i_std in out1_std[1:2]], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited_i).item():.2f}, V_{{target}}= {volume_estimates(out1_i_std).item() * (1+target_volume_increment):.2f})$\" for (out1_i_std, x_edited_i) in zip(out1_std[1:2], x_edited)],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dddacfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = -0.57\n",
    "t_optim_idx = 7\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std[0:1], \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std[0:1]) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"zero\",\n",
    "    opt_kwargs={\"lr\":1e-1}, \n",
    "\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[0:1], \n",
    "        preprocessor1.destandardize(x_edited), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_i_std).item():.2f}$)\" for out1_i_std in out1_std[0:1]], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited_i).item():.2f}, V_{{target}}= {volume_estimates(out1_i_std).item() * (1+target_volume_increment):.2f})$\" for (out1_i_std, x_edited_i) in zip(out1_std[0:1], x_edited)],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d72ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = -0.57\n",
    "t_optim_idx = 8\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std[0:1], \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std[0:1]) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"zero\",\n",
    "    opt_kwargs={\"lr\":1e-1}, \n",
    "\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[0:1], \n",
    "        preprocessor1.destandardize(x_edited), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_i_std).item():.2f}$)\" for out1_i_std in out1_std[0:1]], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited_i).item():.2f}, V_{{target}}= {volume_estimates(out1_i_std).item() * (1+target_volume_increment):.2f})$\" for (out1_i_std, x_edited_i) in zip(out1_std[0:1], x_edited)],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92087875",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = -0.57\n",
    "t_optim_idx = 8\n",
    "x_edited = ddpm_sampler1.ddim_sample_noise_guidance(\n",
    "    model1, \n",
    "    x_0=out1_std[0:1], \n",
    "    obj_fn=volume_estimates_loss_fn, \n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std[0:1]) * (1+target_volume_increment)},\n",
    "    from_t_optim_idx=t_optim_idx, \n",
    "    tgt_noise_level=\"zero\",\n",
    "    opt_kwargs={\"lr\":1, \"decay_fn\": lambda x: th.exp(-x)}, \n",
    "\n",
    ")\n",
    "\n",
    "#plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1[0:1], \n",
    "        preprocessor1.destandardize(x_edited), \n",
    "    ], \n",
    "    title = f\"Optimization from the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_i_std).item():.2f}$)\" for out1_i_std in out1_std[0:1]], \n",
    "        [f\"Edited ($V_{{edit}} = {volume_estimates(x_edited_i).item():.2f}, V_{{target}}= {volume_estimates(out1_i_std).item() * (1+target_volume_increment):.2f})$\" for (out1_i_std, x_edited_i) in zip(out1_std[0:1], x_edited)],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428f4dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = -0.61\n",
    "t_optim_idx = 5\n",
    "x_edited, x_t, x_t_optim = ddpm_sampler1.ddim_sample_latent_optimization(\n",
    "    model1, \n",
    "    x_0=out1_std, \n",
    "    obj_fn=volume_estimates_loss_fn,\n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std) * (1+target_volume_increment)},\n",
    "    t_optim_idx=t_optim_idx,\n",
    "    tgt_noise_level=\"zero\",\n",
    "#    max_opt_iters=50,\n",
    "    loss_threshold=20,\n",
    "    #opt_kwargs={\"lr\":1e-3, \"weight_decay\": 1e-2}, # AdamW default # Too slow with lr=1e-3\n",
    "    opt_kwargs={\"lr\":1e-2, \"weight_decay\": 1e-2}, \n",
    ").values()\n",
    "\n",
    "plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1, \n",
    "        preprocessor1.destandardize(x_edited), \n",
    "        # th.abs(out1- preprocessor1.destandardize(x_edited))\n",
    "    ], \n",
    "    title = f\"Optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        # [f\"Absolute difference between original and edited\" for i in range(out1.shape[0])]\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3637a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = -0.61\n",
    "t_optim_idx = 3\n",
    "x_edited, x_t, x_t_optim = ddpm_sampler1.ddim_sample_latent_optimization(\n",
    "    model1, \n",
    "    x_0=out1_std, \n",
    "    obj_fn=volume_estimates_loss_fn,\n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std) * (1+target_volume_increment), \"max_volume\":1},\n",
    "    t_optim_idx=t_optim_idx,\n",
    "    tgt_noise_level=\"zero\",\n",
    "#    max_opt_iters=50,\n",
    "    loss_threshold=20,\n",
    "    opt_kwargs={\"lr\":1e-2, \"weight_decay\": 1e-3},\n",
    ").values()\n",
    "\n",
    "plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[out1, preprocessor1.destandardize(x_edited), th.abs(out1- preprocessor1.destandardize(x_edited))], \n",
    "    title = f\"Optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        [f\"Absolute difference between original and edited\" for i in range(out1.shape[0])]\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33d4e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, out in enumerate(preprocessor1.destandardize(x_edited)):\n",
    "#     save_sdf_as_mesh(f\"gen32_{i}_v40%incr_3steps.obj\", out, safe=True)\n",
    "\n",
    "# lr_cond_edit = F.interpolate(preprocessor1.destandardize(x_edited), (64, 64, 64), mode=\"nearest\")\n",
    "# lr_cond_edit = preprocessor2.standardize(lr_cond_edit, 0)\n",
    "# x_edited_sr = ddpm_sampler2.sample_ddim(lambda x, t: model2(th.cat([lr_cond_edit, x], 1), t), (out1.shape[0], 1, 64, 64, 64), show_pbar=True)\n",
    "\n",
    "# x_edited_sr = preprocessor2.destandardize(x_edited_sr, 1)\n",
    "\n",
    "# for i, out in enumerate(x_edited_sr):\n",
    "#     save_sdf_as_mesh(f\"sr64_{i}_v40%incr_3steps.obj\", out, safe=True)\n",
    "\n",
    "# plot_sdfs(\n",
    "#     sdfs=[out2, x_edited_sr], \n",
    "#     title = f\"Super-resolution results of optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "#     titles=[\n",
    "#         [f\"Original (SR) ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "#         [f\"Edited (SR) ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "#         ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f305f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = -0.61\n",
    "t_optim_idx = 6\n",
    "x_edited, x_t, x_t_optim = ddpm_sampler1.ddim_sample_latent_optimization(\n",
    "    model1, \n",
    "    x_0=out1_std, \n",
    "    obj_fn=volume_estimates_loss_fn,\n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std) * (1+target_volume_increment), \"max_volume\":1},\n",
    "    t_optim_idx=t_optim_idx,\n",
    "    tgt_noise_level=\"zero\",\n",
    "#    max_opt_iters=50,\n",
    "    loss_threshold=20,\n",
    "    opt_kwargs={\"lr\":1e-2, \"weight_decay\": 1e-2},\n",
    ").values()\n",
    "\n",
    "plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1, \n",
    "        preprocessor1.destandardize(x_edited), \n",
    "        # th.abs(out1- preprocessor1.destandardize(x_edited))\n",
    "    ], \n",
    "    title = f\"Optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        # [f\"Absolute difference between original and edited\" for i in range(out1.shape[0])]\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0b772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_volume_increment = -0.61\n",
    "t_optim_idx = 5\n",
    "x_edited, x_t, x_t_optim = ddpm_sampler1.ddim_sample_latent_optimization(\n",
    "    model1, \n",
    "    x_0=out1_std, \n",
    "    obj_fn=volume_estimates_loss_fn,\n",
    "    obj_fn_args={\"target_volumes\": volume_estimates(out1_std) * (1+target_volume_increment), \"max_volume\":1},\n",
    "    t_optim_idx=t_optim_idx,\n",
    "    tgt_noise_level=\"zero\",\n",
    "#    max_opt_iters=50,\n",
    "    loss_threshold=20,\n",
    "    opt_kwargs={\"lr\":1e-2, \"weight_decay\": 0},\n",
    ").values()\n",
    "\n",
    "plot_sdfs(sdfs=[x_t, x_t_optim], title=f\"Latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$\", titles=[\"Latent to be optimized\", \"Optimized latent\"]) \n",
    "\n",
    "plot_sdfs(\n",
    "    sdfs=[\n",
    "        out1, \n",
    "        preprocessor1.destandardize(x_edited), \n",
    "        # th.abs(out1- preprocessor1.destandardize(x_edited))\n",
    "    ], \n",
    "    title = f\"Optimization on the latent at $t={ddpm_sampler1.ddim_timesteps[t_optim_idx]}$ based on the volume at $t=0$\",\n",
    "    titles=[\n",
    "        [f\"Original ($V = {volume_estimates(out1_std)[i].item():.2f}$)\" for i in range(out1.shape[0])], \n",
    "        [f\"Edited ($V_E = {volume_estimates(x_edited)[i].item():.2f} - V_{{target}}: {volume_estimates(out1_std)[i].item() * (1+target_volume_increment):.2f})$\" for i in range(out1.shape[0])],\n",
    "        # [f\"Absolute difference between original and edited\" for i in range(out1.shape[0])]\n",
    "        ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
